{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('../')\n",
    "from deepod.models.time_series import COUTA, TimesNet, DeepSVDDTS,TranAD,AnomalyTransformer,USAD,DeepIsolationForestTS,GDeepSVDDTS\n",
    "from deepod.metrics import ts_metrics\n",
    "from deepod.metrics import point_adjustment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "ensemble size: 1\n",
      "TcnAE(\n",
      "  (l1): Linear(in_features=512, out_features=32, bias=False)\n",
      "  (encoder): Sequential(\n",
      "    (0): TcnResidualBlock(\n",
      "      (conv1): Conv1d(1, 512, kernel_size=(2,), stride=(1,), padding=(1,), bias=False)\n",
      "      (chomp1): Chomp1d()\n",
      "      (act1): GELU(approximate='none')\n",
      "      (dropout1): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(2,), stride=(1,), padding=(1,), bias=False)\n",
      "      (chomp2): Chomp1d()\n",
      "      (act2): GELU(approximate='none')\n",
      "      (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Conv1d(1, 512, kernel_size=(2,), stride=(1,), padding=(1,), bias=False)\n",
      "        (1): Chomp1d()\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "        (4): Conv1d(512, 512, kernel_size=(2,), stride=(1,), padding=(1,), bias=False)\n",
      "        (5): Chomp1d()\n",
      "        (6): GELU(approximate='none')\n",
      "        (7): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (downsample): Conv1d(1, 512, kernel_size=(1,), stride=(1,))\n",
      "      (act): GELU(approximate='none')\n",
      "    )\n",
      "    (1): TcnResidualBlock(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
      "      (chomp1): Chomp1d()\n",
      "      (act1): GELU(approximate='none')\n",
      "      (dropout1): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
      "      (chomp2): Chomp1d()\n",
      "      (act2): GELU(approximate='none')\n",
      "      (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
      "        (1): Chomp1d()\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "        (4): Conv1d(512, 512, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
      "        (5): Chomp1d()\n",
      "        (6): GELU(approximate='none')\n",
      "        (7): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (act): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): TcnResidualBlockTranspose(\n",
      "      (conv1): ConvTranspose1d(512, 512, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
      "      (pad1): Pad1d()\n",
      "      (act1): GELU(approximate='none')\n",
      "      (dropout1): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): ConvTranspose1d(512, 512, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
      "      (pad2): Pad1d()\n",
      "      (act2): GELU(approximate='none')\n",
      "      (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Dropout(p=0.2, inplace=False)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Pad1d()\n",
      "        (3): ConvTranspose1d(512, 512, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "        (5): GELU(approximate='none')\n",
      "        (6): Pad1d()\n",
      "        (7): ConvTranspose1d(512, 512, kernel_size=(2,), stride=(1,), padding=(2,), dilation=(2,), bias=False)\n",
      "      )\n",
      "      (act): GELU(approximate='none')\n",
      "    )\n",
      "    (1): TcnResidualBlockTranspose(\n",
      "      (conv1): ConvTranspose1d(512, 1, kernel_size=(2,), stride=(1,), padding=(1,), bias=False)\n",
      "      (pad1): Pad1d()\n",
      "      (act1): GELU(approximate='none')\n",
      "      (dropout1): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): ConvTranspose1d(1, 1, kernel_size=(2,), stride=(1,), padding=(1,), bias=False)\n",
      "      (pad2): Pad1d()\n",
      "      (act2): GELU(approximate='none')\n",
      "      (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Dropout(p=0.2, inplace=False)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Pad1d()\n",
      "        (3): ConvTranspose1d(512, 1, kernel_size=(2,), stride=(1,), padding=(1,), bias=False)\n",
      "        (4): Dropout(p=0.2, inplace=False)\n",
      "        (5): GELU(approximate='none')\n",
      "        (6): Pad1d()\n",
      "        (7): ConvTranspose1d(1, 1, kernel_size=(2,), stride=(1,), padding=(1,), bias=False)\n",
      "      )\n",
      "      (downsample): ConvTranspose1d(512, 1, kernel_size=(1,), stride=(1,))\n",
      "      (act): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      ")\n",
      "epoch  1, training loss: 48.398286, time: 0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iot/anaconda3/envs/compare/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, training loss: 41.289228, time: 0.1s\n",
      "epoch 20, training loss: 24.443107, time: 0.1s\n",
      "epoch 30, training loss: 19.217909, time: 0.2s\n",
      "epoch 40, training loss: 14.580994, time: 0.2s\n",
      "epoch 50, training loss: 12.068246, time: 0.1s\n",
      "epoch 60, training loss: 9.610791, time: 0.2s\n",
      "epoch 70, training loss: 8.801938, time: 0.2s\n",
      "Start Inference on the training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing: 100%|█████████████████████████████████████| 47/47 [00:00<00:00, 672.28it/s]\n",
      "testing: 100%|████████████████████████████████████| 78/78 [00:00<00:00, 1117.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.841535948131798, 0.758316091012462, 0.7757304631031829, 0.9504504504504504, 0.65527950310559)\n",
      "(0.867378613920116, 0.7827694133406005, 0.8150825674961042, 1.0, 0.687888198757764)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = 0\n",
    "if dataset==0:\n",
    "    train_data = np.load('../../../TranAD/processed/SWaT/train.npy')\n",
    "    test_data  = np.load('../../../TranAD/processed/SWaT/test.npy')\n",
    "    labels = np.load('../../../TranAD/processed/SWaT/labels.npy')\n",
    "elif dataset==1:\n",
    "    train_data = np.load('/home/iot/GSVDD/TranAD/processed/SMAP/P-1_train.npy')\n",
    "    test_data = np.load('/home/iot/GSVDD/TranAD/processed/SMAP/P-1_test.npy')\n",
    "    labels = np.load('/home/iot/GSVDD/TranAD/processed/SMAP/P-1_labels.npy')[:,0]\n",
    "elif dataset==2:\n",
    "    train_data = np.load('/home/iot/GSVDD/TranAD/processed/MSL/C-1_train.npy')\n",
    "    test_data = np.load('/home/iot/GSVDD/TranAD/processed/MSL/C-1_test.npy')\n",
    "    labels = np.load('/home/iot/GSVDD/TranAD/processed/MSL/C-1_labels.npy')[:,0]\n",
    "\n",
    "clf_all = GDeepSVDDTS(epochs=70,network='TCN')\n",
    "clf_all.fit(train_data)\n",
    "scores_all        = clf_all.decision_function(test_data)\n",
    "eval_metrics_all = ts_metrics(labels, scores_all)\n",
    "result_all = eval_metrics_all\n",
    "adj_eval_metrics = ts_metrics(labels, point_adjustment(labels, scores_all))\n",
    "print(result_all)\n",
    "print(adj_eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating DeepSVDDTS...\n",
      "Start Training...\n",
      "ensemble size: 1\n",
      "{'n_features': 55, 'n_hidden': '512', 'n_output': 64, 'activation': 'GELU', 'bias': False}\n",
      "TCNnet(\n",
      "  (network): Sequential(\n",
      "    (0): TcnResidualBlock(\n",
      "      (conv1): Conv1d(55, 512, kernel_size=(2,), stride=(1,), padding=(1,), bias=False)\n",
      "      (chomp1): Chomp1d()\n",
      "      (act1): GELU(approximate='none')\n",
      "      (dropout1): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(2,), stride=(1,), padding=(1,), bias=False)\n",
      "      (chomp2): Chomp1d()\n",
      "      (act2): GELU(approximate='none')\n",
      "      (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      (net): Sequential(\n",
      "        (0): Conv1d(55, 512, kernel_size=(2,), stride=(1,), padding=(1,), bias=False)\n",
      "        (1): Chomp1d()\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "        (4): Conv1d(512, 512, kernel_size=(2,), stride=(1,), padding=(1,), bias=False)\n",
      "        (5): Chomp1d()\n",
      "        (6): GELU(approximate='none')\n",
      "        (7): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (downsample): Conv1d(55, 512, kernel_size=(1,), stride=(1,))\n",
      "      (act): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (l1): Linear(in_features=512, out_features=64, bias=False)\n",
      ")\n",
      "epoch  1, training loss: 0.456792, time: 0.0s\n",
      "epoch 10, training loss: 0.384362, time: 0.0s\n",
      "epoch 20, training loss: 0.313677, time: 0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iot/anaconda3/envs/compare/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30, training loss: 0.252144, time: 0.0s\n",
      "epoch 40, training loss: 0.199315, time: 0.0s\n",
      "epoch 50, training loss: 0.152707, time: 0.0s\n",
      "epoch 60, training loss: 0.116261, time: 0.0s\n",
      "epoch 70, training loss: 0.086188, time: 0.0s\n",
      "epoch 80, training loss: 0.063860, time: 0.0s\n",
      "epoch 90, training loss: 0.045581, time: 0.0s\n",
      "epoch100, training loss: 0.033675, time: 0.0s\n",
      "Start Inference on the training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing: 100%|█████████████████████████████████████| 34/34 [00:00<00:00, 891.84it/s]\n",
      "testing: 100%|████████████████████████████████████| 35/35 [00:00<00:00, 1516.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating DeepIsolationForestTS...\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 50/50 [00:02<00:00, 19.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Inference on the training data...\n",
      "Start Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 50/50 [00:03<00:00, 15.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 50/50 [00:03<00:00, 15.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating USAD...\n",
      "epoch  1, training loss: 0.058644, time: 0.3\n",
      "epoch 10, training loss: 0.001062, time: 0.3\n",
      "epoch 20, training loss: 0.000531, time: 0.3\n",
      "epoch 30, training loss: 0.000354, time: 0.3\n",
      "epoch 40, training loss: 0.000265, time: 0.3\n",
      "epoch 50, training loss: 0.000212, time: 0.3\n",
      "epoch 60, training loss: 0.000177, time: 0.3\n",
      "epoch 70, training loss: 0.000152, time: 0.3\n",
      "epoch 80, training loss: 0.000133, time: 0.3\n",
      "epoch 90, training loss: 0.000118, time: 0.3\n",
      "epoch100, training loss: 0.000106, time: 0.3\n",
      "Training and evaluating TimesNet...\n",
      "epoch  1, training loss: 0.005166, time: 0.7s\n",
      "epoch  2, training loss: 0.004521, time: 0.7s\n",
      "epoch  3, training loss: 0.004105, time: 0.7s\n",
      "epoch  4, training loss: 0.003473, time: 0.7s\n",
      "epoch  5, training loss: 0.002946, time: 0.7s\n",
      "epoch  6, training loss: 0.002817, time: 0.7s\n",
      "epoch  7, training loss: 0.002712, time: 0.7s\n",
      "epoch  8, training loss: 0.002493, time: 0.7s\n",
      "epoch  9, training loss: 0.002187, time: 0.7s\n",
      "epoch 10, training loss: 0.002182, time: 0.7s\n",
      "Training and evaluating TranAD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iot/anaconda3/envs/compare/lib/python3.8/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\t L1 = 0.0633053967373117\n",
      "Epoch 2,\t L1 = 0.027813791297376156\n",
      "Epoch 3,\t L1 = 0.004913295701365261\n",
      "Epoch 4,\t L1 = 0.0047201700645553716\n",
      "Epoch 5,\t L1 = 0.005002755438908935\n",
      "Training and evaluating COUTA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iot/anaconda3/envs/compare/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|>>> epoch: 01  |   loss: 0.132834, loss_oc: 0.042611, val_loss: 0.038671\n",
      "|>>> epoch: 02  |   loss: 0.123348, loss_oc: 0.035357, val_loss: 0.032080\n",
      "|>>> epoch: 03  |   loss: 0.115276, loss_oc: 0.029345, val_loss: 0.026705\n",
      "|>>> epoch: 04  |   loss: 0.108140, loss_oc: 0.024470, val_loss: 0.022340\n",
      "|>>> epoch: 05  |   loss: 0.101990, loss_oc: 0.020493, val_loss: 0.018769\n",
      "|>>> epoch: 06  |   loss: 0.096279, loss_oc: 0.017243, val_loss: 0.015832\n",
      "|>>> epoch: 07  |   loss: 0.090835, loss_oc: 0.014557, val_loss: 0.013396\n",
      "|>>> epoch: 08  |   loss: 0.086297, loss_oc: 0.012313, val_loss: 0.011300\n",
      "|>>> epoch: 09  |   loss: 0.081529, loss_oc: 0.010341, val_loss: 0.009479\n",
      "|>>> epoch: 10  |   loss: 0.076903, loss_oc: 0.008711, val_loss: 0.008023\n",
      "|>>> epoch: 11  |   loss: 0.073397, loss_oc: 0.007390, val_loss: 0.006818\n",
      "|>>> epoch: 12  |   loss: 0.068603, loss_oc: 0.006272, val_loss: 0.005749\n",
      "|>>> epoch: 13  |   loss: 0.064534, loss_oc: 0.005238, val_loss: 0.004737\n",
      "|>>> epoch: 14  |   loss: 0.061067, loss_oc: 0.004285, val_loss: 0.003828\n",
      "|>>> epoch: 15  |   loss: 0.057568, loss_oc: 0.003432, val_loss: 0.003026\n",
      "|>>> epoch: 16  |   loss: 0.054524, loss_oc: 0.002726, val_loss: 0.002390\n",
      "|>>> epoch: 17  |   loss: 0.050399, loss_oc: 0.002164, val_loss: 0.001894\n",
      "|>>> epoch: 18  |   loss: 0.048319, loss_oc: 0.001720, val_loss: 0.001493\n",
      "|>>> epoch: 19  |   loss: 0.045805, loss_oc: 0.001366, val_loss: 0.001173\n",
      "|>>> epoch: 20  |   loss: 0.043701, loss_oc: 0.001088, val_loss: 0.000919\n",
      "|>>> epoch: 21  |   loss: 0.040026, loss_oc: 0.000866, val_loss: 0.000725\n",
      "|>>> epoch: 22  |   loss: 0.039742, loss_oc: 0.000699, val_loss: 0.000579\n",
      "|>>> epoch: 23  |   loss: 0.037210, loss_oc: 0.000573, val_loss: 0.000474\n",
      "|>>> epoch: 24  |   loss: 0.034942, loss_oc: 0.000484, val_loss: 0.000399\n",
      "|>>> epoch: 25  |   loss: 0.033852, loss_oc: 0.000420, val_loss: 0.000344\n",
      "|>>> epoch: 26  |   loss: 0.032118, loss_oc: 0.000371, val_loss: 0.000301\n",
      "|>>> epoch: 27  |   loss: 0.029968, loss_oc: 0.000339, val_loss: 0.000272\n",
      "|>>> epoch: 28  |   loss: 0.027884, loss_oc: 0.000309, val_loss: 0.000246\n",
      "|>>> epoch: 29  |   loss: 0.027833, loss_oc: 0.000284, val_loss: 0.000227\n",
      "|>>> epoch: 30  |   loss: 0.026953, loss_oc: 0.000266, val_loss: 0.000211\n",
      "|>>> epoch: 31  |   loss: 0.025490, loss_oc: 0.000247, val_loss: 0.000195\n",
      "|>>> epoch: 32  |   loss: 0.026797, loss_oc: 0.000228, val_loss: 0.000179\n",
      "|>>> epoch: 33  |   loss: 0.024387, loss_oc: 0.000213, val_loss: 0.000170\n",
      "|>>> epoch: 34  |   loss: 0.024339, loss_oc: 0.000203, val_loss: 0.000160\n",
      "|>>> epoch: 35  |   loss: 0.020961, loss_oc: 0.000191, val_loss: 0.000153\n",
      "|>>> epoch: 36  |   loss: 0.023406, loss_oc: 0.000178, val_loss: 0.000140\n",
      "|>>> epoch: 37  |   loss: 0.022757, loss_oc: 0.000164, val_loss: 0.000132\n",
      "|>>> epoch: 38  |   loss: 0.022306, loss_oc: 0.000155, val_loss: 0.000124\n",
      "|>>> epoch: 39  |   loss: 0.021554, loss_oc: 0.000146, val_loss: 0.000118\n",
      "|>>> epoch: 40  |   loss: 0.022071, loss_oc: 0.000137, val_loss: 0.000110\n",
      "Training and evaluating AnomalyTransformer...\n",
      "epoch  1, training loss: -23.626032, time: 0.5s\n",
      "epoch  2, training loss: -34.739532, time: 0.4s\n",
      "epoch  3, training loss: -42.301700, time: 0.4s\n",
      "epoch  4, training loss: -45.494680, time: 0.5s\n",
      "epoch  5, training loss: -46.610106, time: 0.5s\n",
      "epoch  6, training loss: -46.992289, time: 0.5s\n",
      "epoch  7, training loss: -47.136799, time: 0.3s\n",
      "epoch  8, training loss: -47.240347, time: 0.4s\n",
      "epoch  9, training loss: -47.329969, time: 0.4s\n",
      "epoch 10, training loss: -47.414707, time: 0.5s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from deepod.models.time_series import (\n",
    "    COUTA, TimesNet, DeepSVDDTS, TranAD, \n",
    "    AnomalyTransformer, USAD, DeepIsolationForestTS\n",
    ")\n",
    "from deepod.metrics import ts_metrics, point_adjustment \n",
    "\n",
    "# Path setup\n",
    "sys.path.append('../')\n",
    "\n",
    "# Data loading\n",
    "dataset = 2\n",
    "if dataset==0:\n",
    "    train_data = np.load('../../../TranAD/processed/SWaT/train.npy')\n",
    "    test_data  = np.load('../../../TranAD/processed/SWaT/test.npy')\n",
    "    labels = np.load('../../../TranAD/processed/SWaT/labels.npy')\n",
    "elif dataset==1:\n",
    "    train_data = np.load('/home/iot/GSVDD/TranAD/processed/SMAP/P-1_train.npy')\n",
    "    test_data = np.load('/home/iot/GSVDD/TranAD/processed/SMAP/P-1_test.npy')\n",
    "    labels = np.load('/home/iot/GSVDD/TranAD/processed/SMAP/P-1_labels.npy')[:,0]\n",
    "elif dataset==2:\n",
    "    train_data = np.load('/home/iot/GSVDD/TranAD/processed/MSL/C-1_train.npy')\n",
    "    test_data = np.load('/home/iot/GSVDD/TranAD/processed/MSL/C-1_test.npy')\n",
    "    labels = np.load('/home/iot/GSVDD/TranAD/processed/MSL/C-1_labels.npy')[:,0]\n",
    "\n",
    "# List of models to evaluate\n",
    "models = {\n",
    "    \"DeepSVDDTS\": DeepSVDDTS(network='TCN'),\n",
    "    \"DeepIsolationForestTS\": DeepIsolationForestTS(),\n",
    "    \"USAD\": USAD(),\n",
    "    \"TimesNet\": TimesNet(),\n",
    "    \"TranAD\": TranAD(),\n",
    "    \"COUTA\": COUTA(),\n",
    "    \"AnomalyTransformer\": AnomalyTransformer(),\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results_summary = {}\n",
    "\n",
    "# Train, predict, and evaluate all models\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(train_data)\n",
    "    \n",
    "    # Evaluate model\n",
    "    scores = model.decision_function(test_data)\n",
    "    eval_metrics = ts_metrics(labels, scores)\n",
    "    adj_metrics = ts_metrics(labels, point_adjustment(labels, scores))\n",
    "    \n",
    "    # Save results\n",
    "    results_summary[model_name] = {\n",
    "        \"Raw Metrics\": eval_metrics,\n",
    "        \"Adjusted Metrics\": adj_metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results Summary:\n",
      "                Model      AUC       F1  Precesion   Recall\n",
      "           DeepSVDDTS 0.997639 0.976373   0.953846 1.000000\n",
      "DeepIsolationForestTS 0.938076 0.784309   1.000000 0.645161\n",
      "                 USAD 0.930094 0.651256   0.482866 1.000000\n",
      "             TimesNet 0.997094 0.974838   0.950920 1.000000\n",
      "               TranAD 0.991812 0.950915   0.906433 1.000000\n",
      "                COUTA 0.999455 0.995180   0.990415 1.000000\n",
      "   AnomalyTransformer 0.983970 0.884446   0.792839 1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Summarize results using pandas DataFrame\n",
    "results_table = []\n",
    "\n",
    "for model_name, metrics in results_summary.items():\n",
    "    adj_metrics = metrics['Adjusted Metrics']  # (precision, recall, f1)\n",
    "    \n",
    "    # Append metrics as a row for the table\n",
    "    results_table.append([\n",
    "        model_name, \n",
    "        adj_metrics[0], adj_metrics[2],adj_metrics[3], adj_metrics[4]\n",
    "    ])\n",
    "\n",
    "# Convert to DataFrame\n",
    "columns = [\n",
    "    \"Model\", \n",
    "    \"AUC\", \"F1\", \"Precesion\",\"Recall\"\n",
    "]\n",
    "df_results = pd.DataFrame(results_table, columns=columns)\n",
    "\n",
    "# Display the table in a clean format\n",
    "print(\"\\nResults Summary:\")\n",
    "print(df_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCN is partially adapted from https://github.com/locuslab/TCN\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils import weight_norm\n",
    "from deepod.core.networks.network_utility import _instantiate_class, _handle_n_hidden\n",
    "\n",
    "\n",
    "class TcnAE(torch.nn.Module):\n",
    "    \"\"\"Temporal Convolutional Network-based AutoEncoder\"\"\"\n",
    "    def __init__(self, n_features, n_hidden='500,100', n_emb=20, activation='ReLU', bias=False,\n",
    "                 kernel_size=2, dropout=0.2,n_output=20):\n",
    "        super(TcnAE, self).__init__()\n",
    "\n",
    "        if type(n_hidden) == int:\n",
    "            n_hidden = [n_hidden]\n",
    "        if type(n_hidden) == str:\n",
    "            n_hidden = n_hidden.split(',')\n",
    "            n_hidden = [int(a) for a in n_hidden]\n",
    "        num_layers = len(n_hidden)\n",
    "\n",
    "        encoder_layers = []\n",
    "        # encoder\n",
    "        for i in range(num_layers+1):\n",
    "            dilation_size = 2 ** i\n",
    "            padding_size = (kernel_size-1) * dilation_size\n",
    "            in_channels = n_features if i == 0 else n_hidden[i-1]\n",
    "            out_channels = n_emb if i == num_layers else n_hidden[i]\n",
    "            encoder_layers += [TcnResidualBlock(in_channels, out_channels, kernel_size,\n",
    "                                                stride=1, dilation=dilation_size,\n",
    "                                                padding=padding_size, dropout=dropout, bias=bias,\n",
    "                                                activation=activation)]\n",
    "            \n",
    "        self.l1 = torch.nn.Linear(out_channels, n_output, bias=bias)\n",
    "        # decoder\n",
    "        decoder_n_hidden = n_hidden[::-1]\n",
    "        decoder_layers = []\n",
    "        for i in range(num_layers+1):\n",
    "            # no dilation in decoder\n",
    "            in_channels = n_emb if i == 0 else decoder_n_hidden[i-1]\n",
    "            out_channels = n_features if i==num_layers else decoder_n_hidden[i]\n",
    "            dilation_size = 2 ** (num_layers-i)\n",
    "            padding_size = (kernel_size-1) * dilation_size\n",
    "            decoder_layers += [TcnResidualBlockTranspose(in_channels, out_channels, kernel_size,\n",
    "                                                         stride=1, dilation=dilation_size,\n",
    "                                                         padding=padding_size, dropout=dropout, bias=bias,\n",
    "                                                         activation=activation)]\n",
    "\n",
    "        # # to register parameters in list of layers, each layer must be an object\n",
    "        # self.enc_layer_names = [\"enc_\" + str(num) for num in range(len(encoder_layers))]\n",
    "        # self.dec_layer_names = [\"dec_\" + str(num) for num in range(len(decoder_layers))]\n",
    "        # for name, layer in zip(self.enc_layer_names, self.encoder_layers):\n",
    "        #     setattr(self, name, layer)\n",
    "        # for name, layer in zip(self.dec_layer_names, self.decoder_layers):\n",
    "        #     setattr(self, name, layer)\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.permute(0, 2, 1)\n",
    "        enc = self.encoder(out)\n",
    "        rep = enc.permute(0,2,1)[:,-1]\n",
    "        rep = self.l1(rep)\n",
    "        dec = self.decoder(enc)\n",
    "        return dec.permute(0, 2, 1), rep\n",
    "\n",
    "\n",
    "class TCNnet(torch.nn.Module):\n",
    "    \"\"\"Temporal Convolutional Network (TCN) for encoding/representing input time series sequences\"\"\"\n",
    "    def __init__(self, n_features, n_hidden='8', n_output=20,\n",
    "                 kernel_size=2, bias=False,\n",
    "                 dropout=0.2, activation='ReLU'):\n",
    "        super(TCNnet, self).__init__()\n",
    "        self.layers = []\n",
    "        self.num_inputs = n_features\n",
    "\n",
    "        if type(n_hidden) == int:\n",
    "            n_hidden = [n_hidden]\n",
    "        if type(n_hidden) == str:\n",
    "            n_hidden = n_hidden.split(',')\n",
    "            n_hidden = [int(a) for a in n_hidden]\n",
    "        num_layers = len(n_hidden)\n",
    "\n",
    "        if dropout is None:\n",
    "            dropout = 0.0\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            dilation_size = 2 ** i\n",
    "            padding_size = (kernel_size-1) * dilation_size\n",
    "            in_channels = n_features if i == 0 else n_hidden[i-1]\n",
    "            out_channels = n_hidden[i]\n",
    "            self.layers += [TcnResidualBlock(in_channels, out_channels, kernel_size,\n",
    "                                             stride=1, dilation=dilation_size,\n",
    "                                             padding=padding_size, dropout=dropout,\n",
    "                                             bias=bias, activation=activation)]\n",
    "        self.network = torch.nn.Sequential(*self.layers)\n",
    "        self.l1 = torch.nn.Linear(n_hidden[-1], n_output, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.network(x.transpose(2, 1)).transpose(2, 1)[:, -1]\n",
    "        rep = self.l1(out)\n",
    "        return rep\n",
    "        # # x shape[bs, seq_len, embed]\n",
    "        # x = x.permute(0, 2, 1)\n",
    "        # out = self.network(x) # output shape is [bs, n_output, seq_len]\n",
    "        # return out.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "class TcnResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding,\n",
    "                 dropout=0.2, activation='ReLU', bias=True):\n",
    "        super(TcnResidualBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = weight_norm(torch.nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                                 stride=stride, padding=padding, bias=bias,\n",
    "                                                 dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.act1 = _instantiate_class(\"torch.nn.modules.activation\", activation)\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(torch.nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                                 stride=stride, padding=padding, bias=bias,\n",
    "                                                 dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.act2 = _instantiate_class(\"torch.nn.modules.activation\", activation)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.net = torch.nn.Sequential(self.conv1, self.chomp1, self.act1, self.dropout1,\n",
    "                                       self.conv2, self.chomp2, self.act2, self.dropout2)\n",
    "        self.downsample = torch.nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.act = _instantiate_class(\"torch.nn.modules.activation\", activation)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape:(bs, embed, seq_len)\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.act(out+res)\n",
    "\n",
    "\n",
    "class TcnResidualBlockTranspose(torch.nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding,\n",
    "                 dropout=0.2, activation='ReLU', bias=False):\n",
    "        super(TcnResidualBlockTranspose, self).__init__()\n",
    "        self.conv1 = weight_norm(torch.nn.ConvTranspose1d(n_inputs, n_outputs, kernel_size,\n",
    "                                                          stride=stride, padding=padding, bias=bias,\n",
    "                                                          dilation=dilation))\n",
    "\n",
    "        self.pad1 = Pad1d(padding)\n",
    "        self.act1 = _instantiate_class(\"torch.nn.modules.activation\", activation)\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(torch.nn.ConvTranspose1d(n_outputs, n_outputs, kernel_size,\n",
    "                                                          stride=stride, padding=padding, bias=bias,\n",
    "                                                          dilation=dilation))\n",
    "        self.pad2 = Pad1d(padding)\n",
    "        self.act2 = _instantiate_class(\"torch.nn.modules.activation\", activation)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.net = torch.nn.Sequential(self.dropout1, self.act1, self.pad1, self.conv1,\n",
    "                                       self.dropout2, self.act2, self.pad2, self.conv2)\n",
    "        self.downsample = torch.nn.ConvTranspose1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.act = _instantiate_class(\"torch.nn.modules.activation\", activation)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.act(out + res)\n",
    "\n",
    "\n",
    "class Pad1d(torch.nn.Module):\n",
    "    def __init__(self, pad_size):\n",
    "        super(Pad1d, self).__init__()\n",
    "        self.pad_size = pad_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([x, x[:, :, -self.pad_size:]], dim = 2).contiguous()\n",
    "\n",
    "\n",
    "class Chomp1d(torch.nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Clipped module, clipped the extra padding\n",
    "        \"\"\"\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing TcnAE...\n",
      "Input shape:  torch.Size([16, 50, 10])\n",
      "Decoded output shape (should match input):  torch.Size([16, 50, 10])\n",
      "Latent embedding shape:  torch.Size([16, 20])\n",
      "TcnAE test passed!\n",
      "\n",
      "Testing TCNnet...\n",
      "Input shape:  torch.Size([16, 50, 10])\n",
      "Output shape:  torch.Size([16, 20])\n",
      "TCNnet test passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Test TcnAE Network\n",
    "def test_tcnae():\n",
    "    print(\"\\nTesting TcnAE...\")\n",
    "    batch_size = 16\n",
    "    seq_len = 50  # Sequence length (time steps)\n",
    "    n_features = 10  # Input features per time step\n",
    "    n_hidden = '8'\n",
    "    n_emb = 20\n",
    "    n_output = 20\n",
    "\n",
    "    # Generate dummy input\n",
    "    x = torch.randn(batch_size, seq_len, n_features)  # Shape: (batch_size, seq_len, n_features)\n",
    "\n",
    "    # Initialize and run TcnAE\n",
    "    model = TcnAE(n_features=n_features, n_hidden=n_hidden, n_emb=n_emb, n_output=n_output)\n",
    "    decoded, embedding = model(x)\n",
    "\n",
    "    # Output shapes\n",
    "    print(\"Input shape: \", x.shape)\n",
    "    print(\"Decoded output shape (should match input): \", decoded.shape)\n",
    "    print(\"Latent embedding shape: \", embedding.shape)\n",
    "\n",
    "    # Shape checks\n",
    "    assert decoded.shape == x.shape, \"Decoded output shape mismatch!\"\n",
    "    assert embedding.shape == (batch_size, n_output), \"Embedding shape mismatch!\"\n",
    "\n",
    "    print(\"TcnAE test passed!\")\n",
    "\n",
    "# Test TCNnet\n",
    "def test_tcnnet():\n",
    "    print(\"\\nTesting TCNnet...\")\n",
    "    batch_size = 16\n",
    "    seq_len = 50  # Sequence length (time steps)\n",
    "    n_features = 10  # Input features per time step\n",
    "    n_hidden = '8'  # Hidden sizes\n",
    "    n_output = 20  # Output size\n",
    "\n",
    "    # Generate dummy input\n",
    "    x = torch.randn(batch_size, seq_len, n_features)  # Shape: (batch_size, seq_len, n_features)\n",
    "\n",
    "    # Initialize and run TCNnet\n",
    "    model = TCNnet(n_features=n_features, n_hidden=n_hidden, n_output=n_output)\n",
    "    output = model(x)\n",
    "\n",
    "    # Output shape\n",
    "    print(\"Input shape: \", x.shape)\n",
    "    print(\"Output shape: \", output.shape)\n",
    "\n",
    "    # Shape checks\n",
    "    assert output.shape == (batch_size, n_output), \"Output shape mismatch!\"\n",
    "\n",
    "    print(\"TCNnet test passed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the tests\n",
    "    test_tcnae()\n",
    "    test_tcnnet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compare",
   "language": "python",
   "name": "compare"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
